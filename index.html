<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Optimization and Gradient Descent</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6;
        }
        .test-your-knowledge h3 {
            color: #28a745;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }
        .faq-section {
            background-color: #fffbea;
        }
        .faq-section h3 {
            color: #ffcc00;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A friendly-looking robot with gears turning in its head, standing at the base of a valley, looking towards the lowest point. The robot is labeled 'Algorithm', and the valley is labeled 'Objective Function'.">
        </div>
        <h1>Introduction to Optimization and Gradient Descent</h1>
        <p>In machine learning, we often talk about 'training' a model. But what does that actually mean? Essentially, we're trying to find the best set of parameters for our model so that it performs well on a given task, like predicting house prices or classifying images. This process of finding the best parameters is called <em>optimization</em>. Today, we'll dive into one of the most fundamental optimization algorithms: <em>Gradient Descent</em>.</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
    <p>Optimization is the heart of machine learning. Without efficient optimization algorithms, training complex models would be impossible. Imagine trying to find the lowest point in a huge, complicated valley by just randomly walking around - that would take forever! Gradient Descent, in its various forms, is the workhorse behind many successful machine learning applications, guiding us efficiently to that lowest point.</p>
        <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
        <p>Before we go further, let's make sure we're all speaking the same language. Here are a few key terms:</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Optimization</h4>
            <p>The process of finding the best set of parameters for a model to minimize (or maximize) a given objective function.</p>
            <h4 class="vocab-term">Parameters</h4>
            <p>Variables in a machine learning model that are adjusted during training.</p>
            <h4 class="vocab-term">Objective Function</h4>
            <p>A function that measures the performance of a model, often representing the error or cost that we want to minimize.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
        <p>Imagine you're standing on a hillside, and you want to get to the lowest point in the valley. One way to do this is to always walk in the direction of the steepest downhill slope. That's essentially what Gradient Descent does. It's an iterative algorithm that adjusts the parameters of our model step by step, always moving in the direction that minimizes the error. But how does it know which direction is 'downhill'?</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A person standing on a hillside, with arrows pointing downwards showing the steepest path to the bottom of the valley. The person is labeled 'Current Parameters', and the arrows are labeled 'Gradient'.">
        </div>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
        <p>To figure out which way is downhill, we need a way to measure how 'wrong' our model is. That's where the <em>cost function</em> comes in. Let's use \( J(\theta) \) to represent the cost function. Our goal is to find the parameters \( \theta \) that minimize \( J(\theta) \). For example in linear regression our cost function might look like this:</p>
        <p>\( J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 \)</p>
        <ul>
            <li>Here, \( m \) is the number of training examples.</li>
            <li>\( h_{\theta}(x^{(i)}) \) is our model's prediction for the i-th example.</li>
            <li>\( y^{(i)} \) is the actual value for the i-th example.</li>
        </ul>
        <p>So, we are calculating the squared difference between our prediction and the actual value, summing those up, and taking the average (scaled by \( \frac{1}{2} \) for mathematical convenience later on). Essentially, the cost function tells us how far our model's predictions are from the actual values. The lower the cost, the better our model is performing. The \( \frac{1}{2} \) is just for mathematical convenience, to make the derivative cleaner.</p>
        <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
        <p>Now, we need to figure out the direction of the steepest descent. That's where the <em>gradient</em> comes in. The gradient is a vector that points in the direction of the greatest increase of the function. We calculate the partial derivatives of \( J(\theta) \) with respect to each parameter in \( \theta \).</p>
        <p>\( \nabla J(\theta) = \begin{bmatrix} \frac{\partial J}{\partial \theta_0} \\ \frac{\partial J}{\partial \theta_1} \\ \vdots \\ \frac{\partial J}{\partial \theta_n} \end{bmatrix} \)</p>
        <ul>
            <li>Each \( \frac{\partial J}{\partial \theta_j} \) represents the partial derivative of the cost function with respect to the parameter \( \theta_j \).</li>
            <li>In simpler terms the gradient tells you, which way is 'up'.</li>
        </ul>
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>Why do we move in the <em>opposite</em> direction of the gradient? What might happen if we moved in the same direction? Discuss.</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
            <p id="stop-and-think-1" style="display: none;">We move in the opposite direction because the gradient points towards the <em>increase</em> of the function, and we want to find the <em>minimum</em>. If we moved in the same direction as the gradient, we'd be climbing up the hill, not going down!</p>
        </div>
        <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        <p>Finally, we update our parameters by taking a step in the opposite direction of the gradient. The size of this step is determined by the <em>learning rate</em>, denoted by \( \alpha \).</p>
        <p>\( \theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} \)</p>
        <ul>
            <li>Here, \( \theta_j \) is the j-th parameter we are updating.</li>
            <li>\( \alpha \) is the learning rate, which controls the size of the step we take.</li>
            <li>\( \frac{\partial J(\theta)}{\partial \theta_j} \) is the partial derivative of the cost function with respect to \( \theta_j \), telling us the slope in the direction of \( \theta_j \).</li>
        </ul>
        <p>So, we're essentially subtracting a fraction of the slope from our current parameter value to move towards the minimum.</p>
        <div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="2D plot of a simple cost function (a parabola). The graph shows a smooth, U-shaped curve with a single lowest point. Along the curve, red arrows indicate the slope (the derivative) at various positions. The arrows are longer where the slope is steeper and shorter where it is shallower, always pointing in the direction of steepest ascent (uphill on the curve). A blue path, depicted by several connected line segments, starts from a high point on the parabola and progresses in the direction opposite to the red arrows, gradually descending toward the minimum. Each segment in the path represents one iteration of gradient descent. A legend explains: “Red Arrows: Gradient (Direction of Steepest Ascent)” and “Blue Path: Gradient Descent Steps.”>
        </div>
        <p>This plot shows a simple cost function. You can see how the gradient arrows point uphill. Our algorithm starts at a point on the surface and takes steps in the opposite direction of the gradient, gradually moving towards the bottom of the bowl.</p>
        <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
        <p>Gradient Descent doesn't find the optimal solution in one shot. It's an iterative process. We start with an initial guess for our parameters and then repeatedly refine them until we reach a minimum (or get close enough). Let's illustrate this with a simple example.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A staircase descending into a valley, with each step labeled 'Iteration 1', 'Iteration 2', etc. A stick figure is walking down the stairs.">
        </div>
        <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
        <h2>Let's Do Some Math</h2>
        <p>Let's say our cost function is \( J(\theta) = \theta^2 \), and our initial guess is \( \theta = 3 \). The learning rate \( \alpha \) is 0.1.</p>
        <p>\( \text{1. Calculate the gradient: } \frac{\partial J(\theta)}{\partial \theta} = 2\theta \)</p>
        <p>\( \text{2. Iteration 1:} \)</p>
        <p>\( \quad \text{Gradient at } \theta = 3: 2 * 3 = 6 \)</p>
        <p>\( \quad \text{Update: } \theta := 3 - 0.1 * 6 = 2.4 \)</p>
        <p>\( \text{3. Iteration 2:} \)</p>
        <p>\( \quad \text{Gradient at } \theta = 2.4: 2 * 2.4 = 4.8 \)</p>
        <p>\( \quad \text{Update: } \theta := 2.4 - 0.1 * 4.8 = 1.92 \)</p>
        <p>\( \text{4. And so on...} \)</p>
        <p>We first calculate the derivative of our simple cost function. In each iteration, we evaluate the gradient at our current \( \theta \) value. Then, we update \( \theta \) by subtracting the learning rate times the gradient.</p>
        <p>You can use this interactive graph to play around with different initial values and learning rates. See how the red dot moves closer to the minimum with each step!</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Interactive graph of \( J(\theta) = \theta^2 \). The graph is a parabola with its minimum at the origin (0,0). There are two sliders: one for the initial value of \( \theta \) (range: -5 to 5) and one for the learning rate \( \alpha \) (range: 0.01 to 0.5). A red dot on the parabola represents the current value of \( \theta \). A 'Step' button is below the graph. When the user clicks 'Step', the red dot moves along the parabola according to one iteration of gradient descent. The calculation for each step is displayed below the button, showing the current \( \theta \), the calculated gradient, and the updated \( \theta \) after the step. The number of steps taken is also displayed (e.g., 'Iteration: 1', 'Iteration: 2', etc.).">
        </div>
        <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
        <p>Let's add a few more terms to our optimization dictionary:</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Gradient</h4>
            <p>A vector containing the partial derivatives of a function, indicating the direction of the steepest ascent.</p>
            <h4 class="vocab-term">Learning Rate \( \alpha \)</h4>
            <p>A hyperparameter that determines the step size taken in the direction opposite to the gradient during each iteration.</p>
            <h4 class="vocab-term">Iteration</h4>
            <p>One complete cycle of calculating the gradient and updating the parameters.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(12)">Continue</div>
    </section>

    <section id="section12">
        <div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>How do we know when to stop the iterations?</h4>
            <p>We typically stop when either the changes in the parameters become very small (convergence) or we reach a predefined maximum number of iterations. We will talk more about convergence in a later lesson.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(13)">Continue</div>
    </section>

    <section id="section13">
        <p>You might be wondering, why go through all this iterative process? Why not just find the minimum directly? Well, for some simple functions, we can. But for complex machine learning models with millions of parameters and huge datasets, calculating the exact solution becomes computationally infeasible. Gradient Descent provides a practical way to find a good solution, even if it might not be the absolute best.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Comparison of computational time vs. dataset size for direct calculation vs. gradient descent. The graph has 'Dataset Size' on the x-axis and 'Computational Time' on the y-axis. There are two lines: one for 'Direct Calculation' and one for 'Gradient Descent'. The 'Direct Calculation' line starts low but curves sharply upwards as dataset size increases, indicating exponential growth. The 'Gradient Descent' line also starts low but increases at a much slower, near-linear rate as dataset size grows. The point where the lines diverge is labeled 'Feasibility Limit for Direct Calculation'. A caption explains: 'Gradient Descent's computational time increases more slowly with dataset size, making it feasible for larger datasets where direct calculation is not practical'.">
        </div>
        <p>As you can see, Gradient Descent's computational cost scales much better with large datasets compared to trying to calculate the solution directly.</p>
        <div class="continue-button" onclick="showNextSection(14)">Continue</div>
    </section>

    <section id="section14">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="'Expectation vs. Reality' meme. Expectation: An elegant mathematical formula written on a chalkboard, representing finding the minimum of a function directly. Reality: A cartoon robot with a bewildered expression, slowly inching its way down a bumpy hill, representing the iterative process of Gradient Descent.">
        </div>
        <p>Sometimes, the reality of machine learning is a bit more... iterative than we might expect!</p>
        <div class="continue-button" onclick="showNextSection(15)">Continue</div>
    </section>

    <section id="section15">
        <h2>Conclusion</h2>
        <p>Great job today! You've learned about optimization, the basics of Gradient Descent, and how to do some of the math behind it. You also saw Gradient Descent visually. In the next lesson we will get our hands dirty with code and dive deeper in the math.</p>
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function revealAnswer(id) {
            const revealText = document.getElementById(id);
            const revealButton = event.target;
            
            revealText.style.display = "block";
            revealButton.style.display = "none";
        }
    </script>
</body>
</html>
